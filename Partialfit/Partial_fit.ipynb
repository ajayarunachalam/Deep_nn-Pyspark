{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(r\"C:\\Users\\reekithak\\Downloads\\drugs.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * from train_set0\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>emb0</th>\n",
       "      <th>emb1</th>\n",
       "      <th>emb2</th>\n",
       "      <th>emb3</th>\n",
       "      <th>emb4</th>\n",
       "      <th>emb5</th>\n",
       "      <th>emb6</th>\n",
       "      <th>emb7</th>\n",
       "      <th>emb8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb23</th>\n",
       "      <th>emb24</th>\n",
       "      <th>emb25</th>\n",
       "      <th>emb26</th>\n",
       "      <th>emb27</th>\n",
       "      <th>emb28</th>\n",
       "      <th>emb29</th>\n",
       "      <th>emb30</th>\n",
       "      <th>emb31</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3081361</td>\n",
       "      <td>4.719912</td>\n",
       "      <td>-1.287492</td>\n",
       "      <td>5.796409</td>\n",
       "      <td>3.760270</td>\n",
       "      <td>-2.509900</td>\n",
       "      <td>-3.106354</td>\n",
       "      <td>6.347282</td>\n",
       "      <td>3.284466</td>\n",
       "      <td>-5.520523</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.252186</td>\n",
       "      <td>-0.474333</td>\n",
       "      <td>5.553354</td>\n",
       "      <td>-3.007781</td>\n",
       "      <td>-6.632124</td>\n",
       "      <td>-2.022637</td>\n",
       "      <td>-5.149702</td>\n",
       "      <td>-5.259633</td>\n",
       "      <td>-5.783206</td>\n",
       "      <td>1D4H,1FLT,1Y6A,1EVT,1IVO,1Y6A,1FLT,1JU5,2XB7,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5327236</td>\n",
       "      <td>1.248603</td>\n",
       "      <td>-0.253252</td>\n",
       "      <td>-0.254269</td>\n",
       "      <td>0.804871</td>\n",
       "      <td>-0.980929</td>\n",
       "      <td>-0.430769</td>\n",
       "      <td>0.525774</td>\n",
       "      <td>-0.733890</td>\n",
       "      <td>-0.735165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211978</td>\n",
       "      <td>-0.483960</td>\n",
       "      <td>-1.107684</td>\n",
       "      <td>0.974848</td>\n",
       "      <td>-0.160632</td>\n",
       "      <td>2.125608</td>\n",
       "      <td>0.882338</td>\n",
       "      <td>-1.016395</td>\n",
       "      <td>1.250610</td>\n",
       "      <td>1D4H,1D4K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5327235</td>\n",
       "      <td>1.409250</td>\n",
       "      <td>-0.664244</td>\n",
       "      <td>-0.052777</td>\n",
       "      <td>1.251166</td>\n",
       "      <td>-0.900441</td>\n",
       "      <td>-0.416737</td>\n",
       "      <td>0.396938</td>\n",
       "      <td>-1.056493</td>\n",
       "      <td>-0.664980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250652</td>\n",
       "      <td>-0.127260</td>\n",
       "      <td>-1.264243</td>\n",
       "      <td>1.083983</td>\n",
       "      <td>-0.324660</td>\n",
       "      <td>2.211021</td>\n",
       "      <td>0.841747</td>\n",
       "      <td>-0.969820</td>\n",
       "      <td>1.200209</td>\n",
       "      <td>1D4H,1D4K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5327234</td>\n",
       "      <td>0.862983</td>\n",
       "      <td>-0.649309</td>\n",
       "      <td>0.103232</td>\n",
       "      <td>0.856554</td>\n",
       "      <td>-0.948067</td>\n",
       "      <td>-0.355748</td>\n",
       "      <td>-0.125225</td>\n",
       "      <td>-0.140954</td>\n",
       "      <td>0.125626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>-0.510167</td>\n",
       "      <td>-0.576433</td>\n",
       "      <td>1.177396</td>\n",
       "      <td>-0.220560</td>\n",
       "      <td>0.872600</td>\n",
       "      <td>0.954063</td>\n",
       "      <td>-0.638187</td>\n",
       "      <td>0.492565</td>\n",
       "      <td>1D4H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3009319</td>\n",
       "      <td>1.577372</td>\n",
       "      <td>-0.625860</td>\n",
       "      <td>0.031868</td>\n",
       "      <td>0.781114</td>\n",
       "      <td>-1.194058</td>\n",
       "      <td>-0.452654</td>\n",
       "      <td>0.164674</td>\n",
       "      <td>-0.567973</td>\n",
       "      <td>-0.532195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213216</td>\n",
       "      <td>-0.307816</td>\n",
       "      <td>-1.132520</td>\n",
       "      <td>0.917929</td>\n",
       "      <td>-0.634182</td>\n",
       "      <td>2.012652</td>\n",
       "      <td>1.027961</td>\n",
       "      <td>-0.934430</td>\n",
       "      <td>1.287832</td>\n",
       "      <td>1D4H,1D4K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509978</th>\n",
       "      <td>91898556</td>\n",
       "      <td>0.295068</td>\n",
       "      <td>0.197552</td>\n",
       "      <td>0.545401</td>\n",
       "      <td>-0.036647</td>\n",
       "      <td>-0.673342</td>\n",
       "      <td>0.732977</td>\n",
       "      <td>-0.100805</td>\n",
       "      <td>-0.109900</td>\n",
       "      <td>-0.150411</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179746</td>\n",
       "      <td>-0.527813</td>\n",
       "      <td>0.696605</td>\n",
       "      <td>0.478272</td>\n",
       "      <td>-0.318172</td>\n",
       "      <td>0.227475</td>\n",
       "      <td>0.437032</td>\n",
       "      <td>-1.003407</td>\n",
       "      <td>0.291253</td>\n",
       "      <td>4P8Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509979</th>\n",
       "      <td>91898557</td>\n",
       "      <td>0.282878</td>\n",
       "      <td>-0.028419</td>\n",
       "      <td>0.690323</td>\n",
       "      <td>0.104092</td>\n",
       "      <td>-0.818978</td>\n",
       "      <td>0.629611</td>\n",
       "      <td>-0.164922</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.337344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.254489</td>\n",
       "      <td>-0.622289</td>\n",
       "      <td>0.656799</td>\n",
       "      <td>0.441361</td>\n",
       "      <td>-0.309904</td>\n",
       "      <td>0.212362</td>\n",
       "      <td>0.625125</td>\n",
       "      <td>-1.108965</td>\n",
       "      <td>0.287895</td>\n",
       "      <td>4P8Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509980</th>\n",
       "      <td>91898558</td>\n",
       "      <td>0.427225</td>\n",
       "      <td>0.084797</td>\n",
       "      <td>0.526330</td>\n",
       "      <td>-0.104751</td>\n",
       "      <td>-0.709305</td>\n",
       "      <td>0.643887</td>\n",
       "      <td>-0.037825</td>\n",
       "      <td>-0.040398</td>\n",
       "      <td>-0.266387</td>\n",
       "      <td>...</td>\n",
       "      <td>1.175247</td>\n",
       "      <td>-0.633522</td>\n",
       "      <td>0.592134</td>\n",
       "      <td>0.494120</td>\n",
       "      <td>-0.343090</td>\n",
       "      <td>0.163036</td>\n",
       "      <td>0.505035</td>\n",
       "      <td>-1.020813</td>\n",
       "      <td>0.345524</td>\n",
       "      <td>4P8Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509981</th>\n",
       "      <td>91898559</td>\n",
       "      <td>0.434695</td>\n",
       "      <td>0.076641</td>\n",
       "      <td>0.547011</td>\n",
       "      <td>0.077848</td>\n",
       "      <td>-0.662812</td>\n",
       "      <td>0.583030</td>\n",
       "      <td>-0.142893</td>\n",
       "      <td>-0.155146</td>\n",
       "      <td>-0.241697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.195542</td>\n",
       "      <td>-0.686928</td>\n",
       "      <td>0.568743</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>-0.410817</td>\n",
       "      <td>0.212401</td>\n",
       "      <td>0.603722</td>\n",
       "      <td>-0.803557</td>\n",
       "      <td>0.082198</td>\n",
       "      <td>4P8Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509982</th>\n",
       "      <td>91898560</td>\n",
       "      <td>0.403091</td>\n",
       "      <td>0.095976</td>\n",
       "      <td>0.542608</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.788175</td>\n",
       "      <td>0.719404</td>\n",
       "      <td>-0.187662</td>\n",
       "      <td>-0.058693</td>\n",
       "      <td>-0.211623</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215766</td>\n",
       "      <td>-0.678801</td>\n",
       "      <td>0.571610</td>\n",
       "      <td>0.440420</td>\n",
       "      <td>-0.509407</td>\n",
       "      <td>0.252154</td>\n",
       "      <td>0.675965</td>\n",
       "      <td>-0.912916</td>\n",
       "      <td>0.367353</td>\n",
       "      <td>4P8Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509983 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cid      emb0      emb1      emb2      emb3      emb4      emb5  \\\n",
       "0        3081361  4.719912 -1.287492  5.796409  3.760270 -2.509900 -3.106354   \n",
       "1        5327236  1.248603 -0.253252 -0.254269  0.804871 -0.980929 -0.430769   \n",
       "2        5327235  1.409250 -0.664244 -0.052777  1.251166 -0.900441 -0.416737   \n",
       "3        5327234  0.862983 -0.649309  0.103232  0.856554 -0.948067 -0.355748   \n",
       "4        3009319  1.577372 -0.625860  0.031868  0.781114 -1.194058 -0.452654   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "509978  91898556  0.295068  0.197552  0.545401 -0.036647 -0.673342  0.732977   \n",
       "509979  91898557  0.282878 -0.028419  0.690323  0.104092 -0.818978  0.629611   \n",
       "509980  91898558  0.427225  0.084797  0.526330 -0.104751 -0.709305  0.643887   \n",
       "509981  91898559  0.434695  0.076641  0.547011  0.077848 -0.662812  0.583030   \n",
       "509982  91898560  0.403091  0.095976  0.542608  0.003101 -0.788175  0.719404   \n",
       "\n",
       "            emb6      emb7      emb8  ...     emb23     emb24     emb25  \\\n",
       "0       6.347282  3.284466 -5.520523  ... -8.252186 -0.474333  5.553354   \n",
       "1       0.525774 -0.733890 -0.735165  ... -0.211978 -0.483960 -1.107684   \n",
       "2       0.396938 -1.056493 -0.664980  ... -0.250652 -0.127260 -1.264243   \n",
       "3      -0.125225 -0.140954  0.125626  ...  0.018531 -0.510167 -0.576433   \n",
       "4       0.164674 -0.567973 -0.532195  ... -0.213216 -0.307816 -1.132520   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "509978 -0.100805 -0.109900 -0.150411  ...  1.179746 -0.527813  0.696605   \n",
       "509979 -0.164922 -0.003292 -0.337344  ...  1.254489 -0.622289  0.656799   \n",
       "509980 -0.037825 -0.040398 -0.266387  ...  1.175247 -0.633522  0.592134   \n",
       "509981 -0.142893 -0.155146 -0.241697  ...  1.195542 -0.686928  0.568743   \n",
       "509982 -0.187662 -0.058693 -0.211623  ...  1.215766 -0.678801  0.571610   \n",
       "\n",
       "           emb26     emb27     emb28     emb29     emb30     emb31  \\\n",
       "0      -3.007781 -6.632124 -2.022637 -5.149702 -5.259633 -5.783206   \n",
       "1       0.974848 -0.160632  2.125608  0.882338 -1.016395  1.250610   \n",
       "2       1.083983 -0.324660  2.211021  0.841747 -0.969820  1.200209   \n",
       "3       1.177396 -0.220560  0.872600  0.954063 -0.638187  0.492565   \n",
       "4       0.917929 -0.634182  2.012652  1.027961 -0.934430  1.287832   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "509978  0.478272 -0.318172  0.227475  0.437032 -1.003407  0.291253   \n",
       "509979  0.441361 -0.309904  0.212362  0.625125 -1.108965  0.287895   \n",
       "509980  0.494120 -0.343090  0.163036  0.505035 -1.020813  0.345524   \n",
       "509981  0.457031 -0.410817  0.212401  0.603722 -0.803557  0.082198   \n",
       "509982  0.440420 -0.509407  0.252154  0.675965 -0.912916  0.367353   \n",
       "\n",
       "                                                   target  \n",
       "0       1D4H,1FLT,1Y6A,1EVT,1IVO,1Y6A,1FLT,1JU5,2XB7,2...  \n",
       "1                                               1D4H,1D4K  \n",
       "2                                               1D4H,1D4K  \n",
       "3                                                    1D4H  \n",
       "4                                               1D4H,1D4K  \n",
       "...                                                   ...  \n",
       "509978                                               4P8Q  \n",
       "509979                                               4P8Q  \n",
       "509980                                               4P8Q  \n",
       "509981                                               4P8Q  \n",
       "509982                                               4P8Q  \n",
       "\n",
       "[509983 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids = df[\"cid\"].unique()\n",
    "unique_pbdid = set(\",\".join(df[\"target\"].values.flatten().tolist()).split(\",\"))\n",
    "pbdid_dict = {v:k for k, v in dict(enumerate(unique_pbdid)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 3/509983 [00:00<5:56:58, 23.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "print(\"Preparing Data\")\n",
    "for cid in tqdm(cids, total=len(cids), leave=False):\n",
    "    indices = np.where(df[\"cid\"] == cid)[0]\n",
    "    xi = df.iloc[indices, 1:-1].values\n",
    "    X.append(xi)\n",
    "    yi = np.zeros(len(pbdid_dict))\n",
    "    targets = \",\".join(df.iloc[indices, -1].values.tolist()).split(\",\")\n",
    "    targets = [pbdid_dict[_id] for _id in targets if _id != \"\"]\n",
    "    yi[targets] = 1\n",
    "    y.append(yi)\n",
    "\n",
    "X = np.array(X).reshape(-1, 32)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "y = np.array(y)\n",
    "\n",
    "np.save('X.npy',X)\n",
    "np.save('y.npy',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509983, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(r'C:\\Users\\reekithak\\train0\\X0.npy') ; y = np.load(r\"C:\\Users\\reekithak\\train0\\y0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((509983, 32), (509983, 2237))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, x_v, y_t, y_v = train_test_split(X, y, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound \n",
    "  \n",
    "freq = 100\n",
    "dur = 50\n",
    "  \n",
    "# loop iterates 5 times i.e, 5 beeps will be produced. \n",
    "for i in range(0, 5):     \n",
    "    winsound.Beep(freq, dur)     \n",
    "    freq+= 100\n",
    "    dur+= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((509983, 32), (509983, 2237))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((494683, 32), (494683, 2237))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.shape , y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2237,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(l, n):\n",
    "    \n",
    "    for i in range(0, len(l), n):\n",
    "        if i + n >= len(l):\n",
    "            yield l[i:len(l)]\n",
    "        else:\n",
    "            yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import tqdm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffledRange = list(range(len(x_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494683"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffledRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER  0\n",
      "Init_Batches =  494683\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (1024, 2237) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-14455192f2c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Batch '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \"\"\"\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    805\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[0;32m    806\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\reekithak\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    845\u001b[0m     raise ValueError(\n\u001b[0;32m    846\u001b[0m         \u001b[1;34m\"y should be a 1d array, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m         \"got an array of shape {} instead.\".format(shape))\n\u001b[0m\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (1024, 2237) instead."
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "for n in range(n_iter):\n",
    "    random.shuffle(shuffledRange)\n",
    "#     x_t = x_t[shuffledRange, :]\n",
    "#     y_t = y_t[shuffledRange, :]\n",
    "    print(\"ITER \",n)\n",
    "    count = 0\n",
    "    print(\"Init_Batches = \",len(shuffledX))\n",
    "    for batch in batches(shuffledRange, 1024):\n",
    "        x_batch = x_t[batch, :]\n",
    "        y_batch = y_t[batch, :]\n",
    "        clf2.partial_fit(x_batch, y_batch)\n",
    "        print('Batch ',count)\n",
    "        count+=1\n",
    "pickle.dump(clf2,open(\"final_model.sav\",'wb'))\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(X, y, bs):\n",
    "    indices = list(range(X.shape[0]))\n",
    "    random.shuffle(indices)\n",
    "    while True:\n",
    "        for i in range(0, len(indices) - (len(indices) % bs), bs):\n",
    "            if i + bs > len(indices):\n",
    "                batch_x = X[i:, :]\n",
    "                batch_y = y[i:, :]\n",
    "            else:\n",
    "                batch_x = X[i:i+bs, :]\n",
    "                batch_y = y[i:i+bs, :]\n",
    "            yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 2237) (1024, 2237)\n"
     ]
    }
   ],
   "source": [
    "# for x_, y_ in batch_gen(X, y, bs=1024):\n",
    "#     yhat = model.predict(x_)\n",
    "#     print(yhat.shape, y_.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(32,), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(2048, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(y.shape[1], activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# model.compile(optimizer=\"adam\", loss=logits_loss, metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "EPOCHS = 50\n",
    "steps_per_epoch = x_t.shape[0] // bs\n",
    "val_steps = x_v.shape[0] // bs\n",
    "train_gen = batch_gen(x_t, y_t, bs=bs)\n",
    "val_gen = batch_gen(x_v, y_v, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "493/493 [==============================] - 92s 188ms/step - loss: 0.0114 - accuracy: 0.0789s - loss: 0.0114 - accuracy: 0.07\n",
      "Epoch 2/50\n",
      "493/493 [==============================] - 93s 188ms/step - loss: 0.0027 - accuracy: 0.3676\n",
      "Epoch 3/50\n",
      "493/493 [==============================] - 92s 187ms/step - loss: 0.0017 - accuracy: 0.5384\n",
      "Epoch 4/50\n",
      "493/493 [==============================] - 93s 188ms/step - loss: 0.0013 - accuracy: 0.6282\n",
      "Epoch 5/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 0.0011 - accuracy: 0.6675\n",
      "Epoch 6/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 9.6528e-04 - accuracy: 0.6958\n",
      "Epoch 7/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 8.8415e-04 - accuracy: 0.7086TA: 34s - loss: 9 - ETA: 29s -  - ETA: 22s - loss: 9.2424e-04 - accuracy: 0 - ETA: 21s - loss: 9.2157e-04 - ac - ETA: 18s - loss: 9.1394e-04  - ETA: 15s \n",
      "Epoch 8/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 8.2297e-04 - accuracy: 0.7193s - loss: 8.2297e-04 - accuracy: 0.71\n",
      "Epoch 9/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 7.8462e-04 - accuracy: 0.7233\n",
      "Epoch 10/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 7.3528e-04 - accuracy: 0.7324\n",
      "Epoch 11/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 7.0714e-04 - accuracy: 0.7339s - loss: 7.1229e-04  - ETA: 0s - loss: 7.0709e-04 - accuracy: 0.\n",
      "Epoch 12/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 6.8118e-04 - accuracy: 0.7388\n",
      "Epoch 13/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 6.5436e-04 - accuracy: 0.7420\n",
      "Epoch 14/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 6.3037e-04 - accuracy: 0.7458\n",
      "Epoch 15/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 6.0089e-04 - accuracy: 0.7484\n",
      "Epoch 16/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 5.7854e-04 - accuracy: 0.7510s - loss: 5.7911e-04 - \n",
      "Epoch 17/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 5.6578e-04 - accuracy: 0.7528\n",
      "Epoch 18/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 5.4200e-04 - accuracy: 0.7574\n",
      "Epoch 19/50\n",
      "493/493 [==============================] - 96s 194ms/step - loss: 5.2933e-04 - accuracy: 0.7588\n",
      "Epoch 20/50\n",
      "493/493 [==============================] - 96s 195ms/step - loss: 5.2517e-04 - accuracy: 0.7598 10s - loss: 5.3121e-04  - ETA: 8s - loss: 5.2 - ETA - ETA: 1s - loss: 5.2516e\n",
      "Epoch 21/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 5.2556e-04 - accuracy: 0.7569\n",
      "Epoch 22/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 5.0812e-04 - accuracy: 0.7586 58s - loss: 5.4462e-04 - accuracy: - ETA: 57s - loss: 5.3898e-04 - - ETA: 54s - loss: 5.3001e-0 - ETA: 50s - loss: 5.2341e-04 - accuracy: - ETA: 40s - loss: 4.927 -\n",
      "Epoch 23/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 4.9153e-04 - accuracy: 0.7606 14s - loss: 4.9625e-04 - - ETA: 11s - loss: 4.9521e-04 - \n",
      "Epoch 24/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 4.7698e-04 - accuracy: 0.7620\n",
      "Epoch 25/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 4.7128e-04 - accuracy: 0.7637\n",
      "Epoch 26/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 4.6304e-04 - accuracy: 0.7652\n",
      "Epoch 27/50\n",
      "493/493 [==============================] - 94s 192ms/step - loss: 4.5214e-04 - accuracy: 0.7651 20s - loss: 4.4902e - ETA: \n",
      "Epoch 28/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 4.4664e-04 - accuracy: 0.7675\n",
      "Epoch 29/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 4.3417e-04 - accuracy: 0.7691\n",
      "Epoch 30/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 4.2789e-04 - accuracy: 0.7691\n",
      "Epoch 31/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 4.2019e-04 - accuracy: 0.7714\n",
      "Epoch 32/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 4.1972e-04 - accuracy: 0.7692\n",
      "Epoch 33/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 4.1485e-04 - accuracy: 0.7698\n",
      "Epoch 34/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 4.1197e-04 - accuracy: 0.7700 20s - loss: 3.9540e-04 - accuracy:  - ETA: 19s - loss: 3.9756e- - ETA: 15s - loss: 4 - ETA: 10s - l - ETA: 6s - loss: 4.119\n",
      "Epoch 35/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 4.0213e-04 - accuracy: 0.7723s - loss: 4.0215e-04 - accuracy - ETA: 3s -\n",
      "Epoch 36/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 4.0171e-04 - accuracy: 0.7713\n",
      "Epoch 37/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 4.0404e-04 - accuracy: 0.7714\n",
      "Epoch 38/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 4.0343e-04 - accuracy: 0.7711\n",
      "Epoch 39/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 3.9748e-04 - accuracy: 0.7719\n",
      "Epoch 40/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 3.9608e-04 - accuracy: 0.7730s - l\n",
      "Epoch 41/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 3.8906e-04 - accuracy: 0.7721 40s - los - ETA: \n",
      "Epoch 42/50\n",
      "493/493 [==============================] - 94s 191ms/step - loss: 3.8558e-04 - accuracy: 0.7727\n",
      "Epoch 43/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 3.8330e-04 - accuracy: 0.7723\n",
      "Epoch 44/50\n",
      "493/493 [==============================] - 93s 190ms/step - loss: 3.8278e-04 - accuracy: 0.7742\n",
      "Epoch 45/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 3.7898e-04 - accuracy: 0.7722\n",
      "Epoch 46/50\n",
      "493/493 [==============================] - 93s 189ms/step - loss: 3.7448e-04 - accuracy: 0.7747\n",
      "Epoch 47/50\n",
      "493/493 [==============================] - 94s 190ms/step - loss: 3.7506e-04 - accuracy: 0.7738\n",
      "Epoch 48/50\n",
      "493/493 [==============================] - 94s 192ms/step - loss: 3.8014e-04 - accuracy: 0.7746s - loss: 3.7755e-04 \n",
      "Epoch 49/50\n",
      "493/493 [==============================] - 95s 193ms/step - loss: 3.7010e-04 - accuracy: 0.7761s - loss: 3.612 - ETA\n",
      "Epoch 50/50\n",
      "493/493 [==============================] - 95s 192ms/step - loss: 3.6654e-04 - accuracy: 0.7766s - loss: 3.630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25819089548>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen, steps_per_epoch = steps_per_epoch, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"graph0-ep50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
